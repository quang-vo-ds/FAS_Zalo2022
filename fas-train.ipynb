{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/vovanquangnbk/fas-train?scriptVersionId=146292922\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","id":"a5b91375","metadata":{"papermill":{"duration":0.005408,"end_time":"2023-10-12T16:08:08.884991","exception":false,"start_time":"2023-10-12T16:08:08.879583","status":"completed"},"tags":[]},"source":["## Set up"]},{"cell_type":"code","execution_count":1,"id":"3c5faf6a","metadata":{"execution":{"iopub.execute_input":"2023-10-12T16:08:08.896331Z","iopub.status.busy":"2023-10-12T16:08:08.896037Z","iopub.status.idle":"2023-10-12T16:08:14.083662Z","shell.execute_reply":"2023-10-12T16:08:14.082743Z"},"papermill":{"duration":5.195906,"end_time":"2023-10-12T16:08:14.085803","exception":false,"start_time":"2023-10-12T16:08:08.889897","status":"completed"},"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n","  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"]}],"source":["from glob import glob\n","from sklearn.model_selection import GroupKFold, StratifiedKFold, GroupShuffleSplit\n","import cv2\n","from skimage import io\n","import torch\n","from torch import nn\n","import os\n","from datetime import datetime\n","import time\n","import random\n","import cv2\n","import torchvision\n","from torchvision import transforms\n","import pandas as pd\n","import numpy as np\n","import math\n","from tqdm import tqdm\n","\n","import matplotlib.pyplot as plt\n","from torch.utils.data import Dataset,DataLoader\n","from torch.utils.data.sampler import SequentialSampler, RandomSampler\n","from torch.cuda.amp import autocast, GradScaler\n","from torch.nn.modules.loss import _WeightedLoss\n","import torch.nn.functional as F\n","from torch.nn import Parameter\n","import torch.utils.model_zoo as model_zoo\n","from torch import nn\n","from torch import optim\n","\n","import sklearn\n","import warnings\n","import joblib\n","from sklearn.metrics import roc_auc_score, log_loss, roc_curve, auc\n","from sklearn import metrics\n","import warnings\n","import cv2"]},{"cell_type":"code","execution_count":2,"id":"a2652f30","metadata":{"execution":{"iopub.execute_input":"2023-10-12T16:08:14.09822Z","iopub.status.busy":"2023-10-12T16:08:14.097827Z","iopub.status.idle":"2023-10-12T16:08:14.166608Z","shell.execute_reply":"2023-10-12T16:08:14.165773Z"},"papermill":{"duration":0.077148,"end_time":"2023-10-12T16:08:14.168425","exception":false,"start_time":"2023-10-12T16:08:14.091277","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Running on device: cuda\n"]}],"source":["device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","print(f'Running on device: {device}')"]},{"cell_type":"code","execution_count":3,"id":"98ce3cbd","metadata":{"execution":{"iopub.execute_input":"2023-10-12T16:08:14.180027Z","iopub.status.busy":"2023-10-12T16:08:14.179424Z","iopub.status.idle":"2023-10-12T16:08:14.184057Z","shell.execute_reply":"2023-10-12T16:08:14.183243Z"},"papermill":{"duration":0.01237,"end_time":"2023-10-12T16:08:14.185786","exception":false,"start_time":"2023-10-12T16:08:14.173416","status":"completed"},"tags":[]},"outputs":[],"source":["data_dir = '/kaggle/input/fas-zalo-2022-frames'\n","metadata_dir = os.path.join(data_dir, \"metadata.csv\")\n","img_dir = os.path.join(data_dir, \"images\")"]},{"cell_type":"code","execution_count":4,"id":"409f80c6","metadata":{"execution":{"iopub.execute_input":"2023-10-12T16:08:14.197272Z","iopub.status.busy":"2023-10-12T16:08:14.196517Z","iopub.status.idle":"2023-10-12T16:08:14.200965Z","shell.execute_reply":"2023-10-12T16:08:14.20018Z"},"papermill":{"duration":0.011869,"end_time":"2023-10-12T16:08:14.202616","exception":false,"start_time":"2023-10-12T16:08:14.190747","status":"completed"},"tags":[]},"outputs":[],"source":["CFG = {\n","    'show_examples': False,\n","    'theta': 0.7,\n","    'seed': 719,\n","    'epochs': 15,\n","    'step_size': 20,\n","    'gamma': 0.5,\n","    'train_bs': 16,\n","    'valid_bs': 1,\n","    'lr': 1e-4,\n","    'weight_decay':5e-5,\n","    'num_workers': 2,\n","}"]},{"cell_type":"code","execution_count":5,"id":"74de7b1c","metadata":{"execution":{"iopub.execute_input":"2023-10-12T16:08:14.214236Z","iopub.status.busy":"2023-10-12T16:08:14.213479Z","iopub.status.idle":"2023-10-12T16:08:14.233417Z","shell.execute_reply":"2023-10-12T16:08:14.232631Z"},"papermill":{"duration":0.02764,"end_time":"2023-10-12T16:08:14.23514","exception":false,"start_time":"2023-10-12T16:08:14.2075","status":"completed"},"tags":[]},"outputs":[],"source":["df = pd.read_csv(metadata_dir)\n","if CFG['show_examples']:\n","    print(df.head())\n","    print(df['liveness_score'].value_counts())"]},{"cell_type":"markdown","id":"a41b87b1","metadata":{"papermill":{"duration":0.004746,"end_time":"2023-10-12T16:08:14.244647","exception":false,"start_time":"2023-10-12T16:08:14.239901","status":"completed"},"tags":[]},"source":["## Utils"]},{"cell_type":"code","execution_count":6,"id":"c64b8c31","metadata":{"execution":{"iopub.execute_input":"2023-10-12T16:08:14.255696Z","iopub.status.busy":"2023-10-12T16:08:14.255194Z","iopub.status.idle":"2023-10-12T16:08:14.260241Z","shell.execute_reply":"2023-10-12T16:08:14.259461Z"},"papermill":{"duration":0.012426,"end_time":"2023-10-12T16:08:14.261957","exception":false,"start_time":"2023-10-12T16:08:14.249531","status":"completed"},"tags":[]},"outputs":[],"source":["def seed_everything(seed):\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = True"]},{"cell_type":"markdown","id":"c1271d98","metadata":{"papermill":{"duration":0.004846,"end_time":"2023-10-12T16:08:14.271624","exception":false,"start_time":"2023-10-12T16:08:14.266778","status":"completed"},"tags":[]},"source":["## Augmentation"]},{"cell_type":"code","execution_count":7,"id":"b6c596a2","metadata":{"execution":{"iopub.execute_input":"2023-10-12T16:08:14.282661Z","iopub.status.busy":"2023-10-12T16:08:14.282182Z","iopub.status.idle":"2023-10-12T16:08:14.286501Z","shell.execute_reply":"2023-10-12T16:08:14.285717Z"},"papermill":{"duration":0.011541,"end_time":"2023-10-12T16:08:14.288196","exception":false,"start_time":"2023-10-12T16:08:14.276655","status":"completed"},"tags":[]},"outputs":[],"source":["seq = transforms.Compose([\n","    transforms.ColorJitter()    \n","])"]},{"cell_type":"markdown","id":"b9536f2f","metadata":{"papermill":{"duration":0.004818,"end_time":"2023-10-12T16:08:14.297794","exception":false,"start_time":"2023-10-12T16:08:14.292976","status":"completed"},"tags":[]},"source":["## Train Dataset"]},{"cell_type":"code","execution_count":8,"id":"32b35f80","metadata":{"execution":{"iopub.execute_input":"2023-10-12T16:08:14.308777Z","iopub.status.busy":"2023-10-12T16:08:14.308538Z","iopub.status.idle":"2023-10-12T16:08:14.32499Z","shell.execute_reply":"2023-10-12T16:08:14.324054Z"},"papermill":{"duration":0.02395,"end_time":"2023-10-12T16:08:14.326612","exception":false,"start_time":"2023-10-12T16:08:14.302662","status":"completed"},"tags":[]},"outputs":[],"source":["class RandomErasing(object):\n","    '''\n","    Class that performs Random Erasing in Random Erasing Data Augmentation by Zhong et al. \n","    -------------------------------------------------------------------------------------\n","    probability: The probability that the operation will be performed.\n","    sl: min erasing area\n","    sh: max erasing area\n","    r1: min aspect ratio\n","    mean: erasing value\n","    -------------------------------------------------------------------------------------\n","    '''\n","    def __init__(self, probability = 0.5, sl = 0.01, sh = 0.05, r1 = 0.5, mean=[0.4914, 0.4822, 0.4465]):\n","        self.probability = probability\n","        self.mean = mean\n","        self.sl = sl\n","        self.sh = sh\n","        self.r1 = r1\n","       \n","    def __call__(self, sample):\n","        img, binary_mask, spoofing_label = sample['image_x'], sample['binary_mask'],sample['spoofing_label']\n","        \n","        if random.uniform(0, 1) < self.probability:\n","            attempts = np.random.randint(1, 3)\n","            for attempt in range(attempts):\n","                area = img.shape[0] * img.shape[1]\n","           \n","                target_area = random.uniform(self.sl, self.sh) * area\n","                aspect_ratio = random.uniform(self.r1, 1/self.r1)\n","    \n","                h = int(round(math.sqrt(target_area * aspect_ratio)))\n","                w = int(round(math.sqrt(target_area / aspect_ratio)))\n","    \n","                if w < img.shape[1] and h < img.shape[0]:\n","                    x1 = random.randint(0, img.shape[0] - h)\n","                    y1 = random.randint(0, img.shape[1] - w)\n","\n","                    img[x1:x1+h, y1:y1+w, 0] = self.mean[0]\n","                    img[x1:x1+h, y1:y1+w, 1] = self.mean[1]\n","                    img[x1:x1+h, y1:y1+w, 2] = self.mean[2]\n","                    \n","        return {'image_x': img, 'binary_mask': binary_mask, 'spoofing_label': spoofing_label}\n","\n","\n","# Tensor\n","class Cutout(object):\n","    def __init__(self, length=50):\n","        self.length = length\n","\n","    def __call__(self, sample):\n","        img, binary_mask, spoofing_label = sample['image_x'], sample['binary_mask'],sample['spoofing_label']\n","        h, w = img.shape[1], img.shape[2]    # Tensor [1][2],  nparray [0][1]\n","        mask = np.ones((h, w), np.float32)\n","        y = np.random.randint(h)\n","        x = np.random.randint(w)\n","        length_new = np.random.randint(1, self.length)\n","        \n","        y1 = np.clip(y - length_new // 2, 0, h)\n","        y2 = np.clip(y + length_new // 2, 0, h)\n","        x1 = np.clip(x - length_new // 2, 0, w)\n","        x2 = np.clip(x + length_new // 2, 0, w)\n","\n","        mask[y1: y2, x1: x2] = 0.\n","        mask = torch.from_numpy(mask)\n","        mask = mask.expand_as(img)\n","        img *= mask\n","        return {'image_x': img, 'binary_mask': binary_mask, 'spoofing_label': spoofing_label}\n","\n","\n","class Normaliztion(object):\n","    \"\"\"\n","        same as mxnet, normalize into [-1, 1]\n","        image = (image - 127.5)/128\n","    \"\"\"\n","    def __call__(self, sample):\n","        image_x, binary_mask, spoofing_label = sample['image_x'], sample['binary_mask'],sample['spoofing_label']\n","        \n","        new_image_x = (image_x - 127.5)/128     # [-1,1]\n","\n","        return {'image_x': new_image_x, 'binary_mask': binary_mask, 'spoofing_label': spoofing_label}\n","\n","\n","class RandomHorizontalFlip(object):\n","    \"\"\"Horizontally flip the given Image randomly with a probability of 0.5.\"\"\"\n","    def __call__(self, sample):\n","        image_x, binary_mask, spoofing_label = sample['image_x'], sample['binary_mask'],sample['spoofing_label']\n","        \n","        new_image_x = np.zeros((256, 256, 3))\n","        new_binary_mask = np.zeros((32, 32))\n","\n","        p = random.random()\n","        if p < 0.5:\n","\n","            new_image_x = cv2.flip(image_x, 1)\n","            new_binary_mask = cv2.flip(binary_mask, 1)\n","           \n","                \n","            return {'image_x': new_image_x, 'binary_mask': new_binary_mask, 'spoofing_label': spoofing_label}\n","        else:\n","            return {'image_x': image_x, 'binary_mask': binary_mask, 'spoofing_label': spoofing_label}\n","\n","\n","\n","class ToTensor(object):\n","    \"\"\"\n","        Convert ndarrays in sample to Tensors.\n","        process only one batch every time\n","    \"\"\"\n","\n","    def __call__(self, sample):\n","        image_x, binary_mask, spoofing_label = sample['image_x'], sample['binary_mask'],sample['spoofing_label']\n","        \n","        # swap color axis because\n","        # numpy image: (batch_size) x H x W x C\n","        # torch image: (batch_size) x C X H X W\n","        image_x = image_x[:,:,::-1].transpose((2, 0, 1))\n","        image_x = np.array(image_x)\n","        \n","        binary_mask = np.array(binary_mask)\n","\n","                        \n","        spoofing_label_np = np.array([0])\n","        spoofing_label_np[0] = spoofing_label\n","        \n","        \n","        return {'image_x': torch.from_numpy(image_x).float(), 'binary_mask': torch.from_numpy(binary_mask).float(), 'spoofing_label': torch.from_numpy(spoofing_label_np).float()}"]},{"cell_type":"code","execution_count":9,"id":"dbe42572","metadata":{"execution":{"iopub.execute_input":"2023-10-12T16:08:14.33766Z","iopub.status.busy":"2023-10-12T16:08:14.337435Z","iopub.status.idle":"2023-10-12T16:08:14.344812Z","shell.execute_reply":"2023-10-12T16:08:14.343963Z"},"papermill":{"duration":0.014744,"end_time":"2023-10-12T16:08:14.346443","exception":false,"start_time":"2023-10-12T16:08:14.331699","status":"completed"},"tags":[]},"outputs":[],"source":["class TrainDataset(Dataset):\n","\n","    def __init__(self, df, root_dir,  transform=None):\n","\n","        self.df = df.copy()\n","        self.root_dir = root_dir\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    \n","    def __getitem__(self, idx):\n","        img_name = str(self.df.iloc[idx, 1])\n","        image_path = os.path.join(self.root_dir, img_name)\n","    \n","        image_x, binary_mask = self.get_single_image_x(image_path)\n","        \n","        spoofing_label = self.df.iloc[idx, 2]\n","        if spoofing_label == 1:\n","            spoofing_label = 1            # real\n","        else:\n","            spoofing_label = 0            # fake\n","            binary_mask = np.zeros((32, 32))     \n","\n","        sample = {'image_x': image_x, 'binary_mask': binary_mask, 'spoofing_label': spoofing_label}\n","\n","        if self.transform:\n","            sample = self.transform(sample)\n","        return sample\n","\n","    \n","    def get_single_image_x(self, image_path):\n","        image_x = np.zeros((256, 256, 3))\n","        binary_mask = np.zeros((32, 32))\n"," \n","        image_x_temp = cv2.imread(image_path)\n","        image_x_temp_gray = cv2.imread(image_path, 0)\n","\n","        image_x = cv2.resize(image_x_temp, (256, 256))\n","        image_x_temp_gray = cv2.resize(image_x_temp_gray, (32, 32))\n","        image_x_aug = image_x\n","#         seq(img=image_x)['image']\n","        \n","        for i in range(32):\n","            for j in range(32):\n","                if image_x_temp_gray[i,j]>0:\n","                    binary_mask[i,j]=1\n","                else:\n","                    binary_mask[i,j]=0\n","        \n","        return image_x_aug, binary_mask"]},{"cell_type":"code","execution_count":10,"id":"9ca25ce4","metadata":{"execution":{"iopub.execute_input":"2023-10-12T16:08:14.357223Z","iopub.status.busy":"2023-10-12T16:08:14.356866Z","iopub.status.idle":"2023-10-12T16:08:14.361334Z","shell.execute_reply":"2023-10-12T16:08:14.360451Z"},"papermill":{"duration":0.0116,"end_time":"2023-10-12T16:08:14.362906","exception":false,"start_time":"2023-10-12T16:08:14.351306","status":"completed"},"tags":[]},"outputs":[],"source":["# Show examples\n","if CFG['show_examples']:\n","    transform = transforms.Compose([RandomErasing(), RandomHorizontalFlip(),  ToTensor(), Cutout(), Normaliztion()])\n","    dataset = TrainDataset(df, img_dir, transform)\n","    for i, sample in enumerate(dataset):\n","\n","        print(sample['image_x'].shape)\n","        print(sample['binary_mask'].shape)\n","        print(sample['spoofing_label'])\n","        \n","        if i > 2:\n","            break"]},{"cell_type":"markdown","id":"1892e947","metadata":{"papermill":{"duration":0.004749,"end_time":"2023-10-12T16:08:14.372556","exception":false,"start_time":"2023-10-12T16:08:14.367807","status":"completed"},"tags":[]},"source":["## Val Dataset"]},{"cell_type":"code","execution_count":11,"id":"fcbd0b45","metadata":{"execution":{"iopub.execute_input":"2023-10-12T16:08:14.383981Z","iopub.status.busy":"2023-10-12T16:08:14.383235Z","iopub.status.idle":"2023-10-12T16:08:14.389864Z","shell.execute_reply":"2023-10-12T16:08:14.389113Z"},"papermill":{"duration":0.014137,"end_time":"2023-10-12T16:08:14.391521","exception":false,"start_time":"2023-10-12T16:08:14.377384","status":"completed"},"tags":[]},"outputs":[],"source":["class Normaliztion_valtest(object):\n","    \"\"\"\n","        same as mxnet, normalize into [-1, 1]\n","        image = (image - 127.5)/128\n","    \"\"\"\n","    def __call__(self, sample):\n","        image_x, binary_mask, spoofing_label = sample['image_x'],sample['binary_mask'] ,sample['spoofing_label']\n","        new_image_x = (image_x - 127.5)/128     # [-1,1]\n","        return {'image_x': new_image_x, 'binary_mask': binary_mask , 'spoofing_label': spoofing_label}\n","\n","\n","class ToTensor_valtest(object):\n","    \"\"\"\n","        Convert ndarrays in sample to Tensors.\n","        process only one batch every time\n","    \"\"\"\n","\n","    def __call__(self, sample):\n","        image_x, binary_mask, spoofing_label = sample['image_x'],sample['binary_mask'] ,sample['spoofing_label']\n","        \n","        # swap color axis because    BGR2RGB\n","        # numpy image: (batch_size) x T x H x W x C\n","        # torch image: (batch_size) x T x C X H X W\n","        image_x = image_x[:,:,:,::-1].transpose((0, 3, 1, 2))\n","        image_x = np.array(image_x)\n","        \n","        binary_mask = np.array(binary_mask)\n","                        \n","        spoofing_label_np = np.array([0])\n","        spoofing_label_np[0] = spoofing_label\n","        \n","        return {'image_x': torch.from_numpy(image_x).float(), 'binary_mask': torch.from_numpy(binary_mask).float(),'spoofing_label': torch.from_numpy(spoofing_label_np).long()}"]},{"cell_type":"code","execution_count":12,"id":"69764d05","metadata":{"execution":{"iopub.execute_input":"2023-10-12T16:08:14.402692Z","iopub.status.busy":"2023-10-12T16:08:14.402203Z","iopub.status.idle":"2023-10-12T16:08:14.410665Z","shell.execute_reply":"2023-10-12T16:08:14.409886Z"},"papermill":{"duration":0.016023,"end_time":"2023-10-12T16:08:14.412349","exception":false,"start_time":"2023-10-12T16:08:14.396326","status":"completed"},"tags":[]},"outputs":[],"source":["class ValDataset(Dataset):\n","    \n","    def __init__(self, df, root_dir,  transform=None):\n","\n","        self.df = df.copy()\n","        self.root_dir = root_dir\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    \n","    def __getitem__(self, idx):\n","\n","        fname = str(self.df.iloc[idx, 0])\n","             \n","        image_x, binary_mask = self.get_single_image_x(fname)\n","        spoofing_label = self.df.iloc[idx, 2]\n","            \n","        sample = {'image_x': image_x, 'binary_mask': binary_mask, 'spoofing_label': spoofing_label}\n","\n","        if self.transform:\n","            sample = self.transform(sample)\n","        return sample\n","\n","    def get_single_image_x(self, fname):\n","\n","        temp_df = self.df[self.df['fname'] == fname]\n","        frames_total = len(temp_df)\n","        \n","        image_x = np.zeros((frames_total, 256, 256, 3))\n","        \n","        binary_mask = np.zeros((frames_total, 32, 32))\n","        \n","        \n","        for ii in range(frames_total):\n","            img_name = str(temp_df.iloc[ii, 1])\n","            img_path = os.path.join(self.root_dir, img_name)\n","            \n","            image_x_temp_gray = cv2.imread(img_path, 0)\n","            image_x_temp_gray = cv2.resize(image_x_temp_gray, (32, 32))\n","\n","            image_x_temp = cv2.imread(img_path)\n","            image_x[ii,:,:,:] = cv2.resize(image_x_temp, (256, 256))\n","\n","            \n","            for i in range(32):\n","                for j in range(32):\n","                    if image_x_temp_gray[i,j]>0:\n","                        binary_mask[ii, i, j]=1.0\n","                    else:\n","                        binary_mask[ii, i, j]=0.0\n","            \n","\n","        \n","        return image_x, binary_mask"]},{"cell_type":"code","execution_count":13,"id":"4948f217","metadata":{"execution":{"iopub.execute_input":"2023-10-12T16:08:14.423548Z","iopub.status.busy":"2023-10-12T16:08:14.4228Z","iopub.status.idle":"2023-10-12T16:08:14.427708Z","shell.execute_reply":"2023-10-12T16:08:14.426963Z"},"papermill":{"duration":0.012166,"end_time":"2023-10-12T16:08:14.42938","exception":false,"start_time":"2023-10-12T16:08:14.417214","status":"completed"},"tags":[]},"outputs":[],"source":["# Show examples\n","if CFG['show_examples']:\n","    transform = transforms.Compose([Normaliztion_valtest(), ToTensor_valtest()])\n","    dataset = ValDataset(df, img_dir, transform)\n","    for i, val_sample in enumerate(dataset):\n","        print(val_sample['image_x'].shape)\n","        print(val_sample['binary_mask'].shape)\n","        print(val_sample['spoofing_label'])\n","        \n","        if i > 3:\n","            break"]},{"cell_type":"markdown","id":"bba50557","metadata":{"papermill":{"duration":0.004805,"end_time":"2023-10-12T16:08:14.439","exception":false,"start_time":"2023-10-12T16:08:14.434195","status":"completed"},"tags":[]},"source":["## Model"]},{"cell_type":"code","execution_count":14,"id":"96968cfa","metadata":{"execution":{"iopub.execute_input":"2023-10-12T16:08:14.44979Z","iopub.status.busy":"2023-10-12T16:08:14.44957Z","iopub.status.idle":"2023-10-12T16:08:14.46764Z","shell.execute_reply":"2023-10-12T16:08:14.466859Z"},"papermill":{"duration":0.026514,"end_time":"2023-10-12T16:08:14.47031","exception":false,"start_time":"2023-10-12T16:08:14.443796","status":"completed"},"tags":[]},"outputs":[],"source":["############\n","## CDC block\n","############\n","class Conv2d_cd(nn.Module):\n","    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1,\n","                 padding=1, dilation=1, groups=1, bias=False, theta=CFG['theta']):\n","\n","        super(Conv2d_cd, self).__init__()\n","        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias)\n","        self.theta = theta\n","\n","    def forward(self, x):\n","        out_normal = self.conv(x)\n","\n","        if math.fabs(self.theta - 0.0) < 1e-8:\n","            return out_normal \n","        else:\n","            #pdb.set_trace()\n","            [C_out,C_in, kernel_size,kernel_size] = self.conv.weight.shape\n","            kernel_diff = self.conv.weight.sum(2).sum(2)\n","            kernel_diff = kernel_diff[:, :, None, None]\n","            out_diff = F.conv2d(input=x, weight=kernel_diff, bias=self.conv.bias, stride=self.conv.stride, padding=0, dilation=self.conv.dilation, groups=self.conv.groups)\n","\n","            return out_normal - self.theta * out_diff\n","\n","############\n","## Attention block\n","############\n","class SpatialAttention(nn.Module):\n","    def __init__(self, kernel = 3):\n","        super(SpatialAttention, self).__init__()\n","\n","\n","        self.conv1 = nn.Conv2d(2, 1, kernel_size=kernel, padding=kernel//2, bias=False)\n","        self.sigmoid = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        avg_out = torch.mean(x, dim=1, keepdim=True)\n","        max_out, _ = torch.max(x, dim=1, keepdim=True)\n","        x = torch.cat([avg_out, max_out], dim=1)\n","        x = self.conv1(x)\n","        \n","        return self.sigmoid(x)\n","\n","############\n","## CDCN architecture\n","############\n","\n","class CDCN(nn.Module):\n","\n","    def __init__(self, basic_conv=Conv2d_cd, theta=CFG['theta']):   \n","        super(CDCN, self).__init__()\n","        \n","        \n","        self.conv1 = nn.Sequential(\n","            basic_conv(3, 80, kernel_size=3, stride=1, padding=1, bias=False, theta=theta),\n","            nn.BatchNorm2d(80),\n","            nn.ReLU(),    \n","            \n","        )\n","        \n","        self.Block1 = nn.Sequential(\n","            basic_conv(80, 160, kernel_size=3, stride=1, padding=1, bias=False, theta= theta),\n","            nn.BatchNorm2d(160),\n","            nn.ReLU(),  \n","            \n","            basic_conv(160, int(160*1.6), kernel_size=3, stride=1, padding=1, bias=False, theta= theta),\n","            nn.BatchNorm2d(int(160*1.6)),\n","            nn.ReLU(),  \n","            basic_conv(int(160*1.6), 160, kernel_size=3, stride=1, padding=1, bias=False, theta= theta),\n","            nn.BatchNorm2d(160),\n","            nn.ReLU(), \n","            \n","            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n","            \n","        )\n","        \n","        self.Block2 = nn.Sequential(\n","            basic_conv(160, int(160*1.2), kernel_size=3, stride=1, padding=1, bias=False, theta= theta),\n","            nn.BatchNorm2d(int(160*1.2)),\n","            nn.ReLU(),  \n","            basic_conv(int(160*1.2), 160, kernel_size=3, stride=1, padding=1, bias=False, theta= theta),\n","            nn.BatchNorm2d(160),\n","            nn.ReLU(),  \n","            basic_conv(160, int(160*1.4), kernel_size=3, stride=1, padding=1, bias=False, theta= theta),\n","            nn.BatchNorm2d(int(160*1.4)),\n","            nn.ReLU(),  \n","            basic_conv(int(160*1.4), 160, kernel_size=3, stride=1, padding=1, bias=False, theta= theta),\n","            nn.BatchNorm2d(160),\n","            nn.ReLU(),  \n","            \n","            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n","        )\n","        \n","        self.Block3 = nn.Sequential(\n","            basic_conv(160, 160, kernel_size=3, stride=1, padding=1, bias=False, theta= theta),\n","            nn.BatchNorm2d(160),\n","            nn.ReLU(), \n","            basic_conv(160, int(160*1.2), kernel_size=3, stride=1, padding=1, bias=False, theta= theta),\n","            nn.BatchNorm2d(int(160*1.2)),\n","            nn.ReLU(),  \n","            basic_conv(int(160*1.2), 160, kernel_size=3, stride=1, padding=1, bias=False, theta= theta),\n","            nn.BatchNorm2d(160),\n","            nn.ReLU(), \n","            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n","        )\n","        \n","        # Original\n","        \n","        self.lastconv1 = nn.Sequential(\n","            basic_conv(160*3, 160, kernel_size=3, stride=1, padding=1, bias=False, theta= theta),\n","            nn.BatchNorm2d(160),\n","            nn.ReLU(),\n","            basic_conv(160, 1, kernel_size=3, stride=1, padding=1, bias=False, theta= theta),\n","            nn.ReLU(),    \n","        )\n","        \n","      \n","        self.sa1 = SpatialAttention(kernel = 7)\n","        self.sa2 = SpatialAttention(kernel = 5)\n","        self.sa3 = SpatialAttention(kernel = 3)\n","        self.downsample32x32 = nn.Upsample(size=(32, 32), mode='bilinear')\n","\n"," \n","    def forward(self, x):\t    \t# x [3, 256, 256]\n","        \n","        x_input = x\n","        x = self.conv1(x)\t\t   \n","        \n","        x_Block1 = self.Block1(x)\t    \t    \t\n","        attention1 = self.sa1(x_Block1)\n","        x_Block1_SA = attention1 * x_Block1\n","        x_Block1_32x32 = self.downsample32x32(x_Block1_SA)   \n","        \n","        x_Block2 = self.Block2(x_Block1)\t    \n","        attention2 = self.sa2(x_Block2)  \n","        x_Block2_SA = attention2 * x_Block2\n","        x_Block2_32x32 = self.downsample32x32(x_Block2_SA)  \n","        \n","        x_Block3 = self.Block3(x_Block2)\t    \n","        attention3 = self.sa3(x_Block3)  \n","        x_Block3_SA = attention3 * x_Block3\t\n","        x_Block3_32x32 = self.downsample32x32(x_Block3_SA)   \n","        \n","        x_concat = torch.cat((x_Block1_32x32,x_Block2_32x32,x_Block3_32x32), dim=1)    \n","        \n","        map_x = self.lastconv1(x_concat)\n","        \n","        map_x = map_x.squeeze(1)\n","        \n","        return map_x, x_concat, attention1, attention2, attention3, x_input"]},{"cell_type":"code","execution_count":15,"id":"fb893893","metadata":{"execution":{"iopub.execute_input":"2023-10-12T16:08:14.495773Z","iopub.status.busy":"2023-10-12T16:08:14.494833Z","iopub.status.idle":"2023-10-12T16:08:14.503477Z","shell.execute_reply":"2023-10-12T16:08:14.502629Z"},"papermill":{"duration":0.023433,"end_time":"2023-10-12T16:08:14.506158","exception":false,"start_time":"2023-10-12T16:08:14.482725","status":"completed"},"tags":[]},"outputs":[],"source":["# Test dataset\n","if CFG['show_examples']:\n","    model = CDCN(basic_conv=Conv2d_cd, theta=CFG['theta'])\n","    inputs, binary_mask, spoof_label = sample['image_x'], sample['binary_mask'], sample['spoofing_label']\n","    map_x, embedding, x_Block1, x_Block2, x_Block3, x_input =  model(inputs.unsqueeze(0))\n","    print(map_x.shape)\n","    print(map_x)\n","    print(binary_mask.shape)\n","    print(binary_mask)"]},{"cell_type":"markdown","id":"4fe997d6","metadata":{"papermill":{"duration":0.008088,"end_time":"2023-10-12T16:08:14.527584","exception":false,"start_time":"2023-10-12T16:08:14.519496","status":"completed"},"tags":[]},"source":["## Loss"]},{"cell_type":"code","execution_count":16,"id":"c81088a3","metadata":{"execution":{"iopub.execute_input":"2023-10-12T16:08:14.542751Z","iopub.status.busy":"2023-10-12T16:08:14.542511Z","iopub.status.idle":"2023-10-12T16:08:14.550192Z","shell.execute_reply":"2023-10-12T16:08:14.549285Z"},"papermill":{"duration":0.014976,"end_time":"2023-10-12T16:08:14.551777","exception":false,"start_time":"2023-10-12T16:08:14.536801","status":"completed"},"tags":[]},"outputs":[],"source":["def contrast_depth_conv(inputs):\n","    ''' compute contrast depth in both of (out, label) '''\n","    '''\n","        input  32x32\n","        output 8x32x32\n","    '''\n","    \n","\n","    kernel_filter_list =[\n","                        [[1,0,0],[0,-1,0],[0,0,0]], [[0,1,0],[0,-1,0],[0,0,0]], [[0,0,1],[0,-1,0],[0,0,0]],\n","                        [[0,0,0],[1,-1,0],[0,0,0]], [[0,0,0],[0,-1,1],[0,0,0]],\n","                        [[0,0,0],[0,-1,0],[1,0,0]], [[0,0,0],[0,-1,0],[0,1,0]], [[0,0,0],[0,-1,0],[0,0,1]]\n","                        ]\n","    \n","    kernel_filter = np.array(kernel_filter_list, np.float32)\n","    \n","    kernel_filter = torch.from_numpy(kernel_filter).float().to(device)\n","    # weights (in_channel, out_channel, kernel, kernel)\n","    kernel_filter = kernel_filter.unsqueeze(dim=1)\n","    \n","    inputs = inputs.unsqueeze(dim=1).expand(inputs.shape[0], 8, inputs.shape[1],inputs.shape[2])\n","    \n","    contrast_depth = F.conv2d(inputs, weight=kernel_filter, groups=8)  # depthwise conv\n","    \n","    return contrast_depth\n","\n","\n","class Contrast_depth_loss(nn.Module):    # Pearson range [-1, 1] so if < 0, abs|loss| ; if >0, 1- loss\n","    def __init__(self):\n","        super(Contrast_depth_loss,self).__init__()\n","        return\n","    def forward(self, out, label): \n","        '''\n","        compute contrast depth in both of (out, label),\n","        then get the loss of them\n","        tf.atrous_convd match tf-versions: 1.4\n","        '''\n","        contrast_out = contrast_depth_conv(out)\n","        contrast_label = contrast_depth_conv(label)\n","        \n","        criterion_MSE = nn.MSELoss().to(device)\n","    \n","        loss = criterion_MSE(contrast_out, contrast_label)\n","    \n","        return loss"]},{"cell_type":"code","execution_count":17,"id":"454416ae","metadata":{"execution":{"iopub.execute_input":"2023-10-12T16:08:14.562888Z","iopub.status.busy":"2023-10-12T16:08:14.56216Z","iopub.status.idle":"2023-10-12T16:08:14.566571Z","shell.execute_reply":"2023-10-12T16:08:14.565812Z"},"papermill":{"duration":0.011671,"end_time":"2023-10-12T16:08:14.568212","exception":false,"start_time":"2023-10-12T16:08:14.556541","status":"completed"},"tags":[]},"outputs":[],"source":["if CFG['show_examples']:\n","    criterion = Contrast_depth_loss()\n","    print(criterion(binary_mask.unsqueeze(0).to(device), map_x.to(device)))"]},{"cell_type":"markdown","id":"c8c31717","metadata":{"papermill":{"duration":0.004751,"end_time":"2023-10-12T16:08:14.577773","exception":false,"start_time":"2023-10-12T16:08:14.573022","status":"completed"},"tags":[]},"source":["## Trainer"]},{"cell_type":"code","execution_count":18,"id":"78ce0a44","metadata":{"execution":{"iopub.execute_input":"2023-10-12T16:08:14.600155Z","iopub.status.busy":"2023-10-12T16:08:14.599585Z","iopub.status.idle":"2023-10-12T16:08:14.639762Z","shell.execute_reply":"2023-10-12T16:08:14.638761Z"},"papermill":{"duration":0.057159,"end_time":"2023-10-12T16:08:14.642754","exception":false,"start_time":"2023-10-12T16:08:14.585595","status":"completed"},"tags":[]},"outputs":[],"source":["## Utils\n","class AvgrageMeter(object):\n","    def __init__(self):\n","        self.reset()\n","        \n","    def reset(self):\n","        self.avg = 0\n","        self.sum = 0\n","        self.cnt = 0\n","        \n","    def update(self, val, n=1):\n","        self.sum += val * n\n","        self.cnt += n\n","        self.avg = self.sum / self.cnt\n","\n","def accuracy(output, target, topk=(1,)):\n","    maxk = max(topk)\n","    batch_size = target.size(0)\n","    \n","    _, pred = output.topk(maxk, 1, True, True)\n","    pred = pred.t()\n","    correct = pred.eq(target.view(1, -1).expand_as(pred))\n","    \n","    res = []\n","    for k in topk:\n","        correct_k = correct[:k].view(-1).float().sum(0)\n","        res.append(correct_k.mul_(100.0/batch_size))\n","    return res\n","\n","def get_threshold(score_file):\n","    with open(score_file, 'r') as file:\n","        lines = file.readlines()\n","\n","    data = []\n","    count = 0.0\n","    num_real = 0.0\n","    num_fake = 0.0\n","    for line in lines:\n","        count += 1\n","        tokens = line.split()\n","        angle = float(tokens[0])\n","        #pdb.set_trace()\n","        type = int(tokens[1])\n","        data.append({'map_score': angle, 'label': type})\n","        if type==1:\n","            num_real += 1\n","        else:\n","            num_fake += 1\n","\n","    min_error = count    # account ACER (or ACC)\n","    min_threshold = 0.0\n","    min_ACC = 0.0\n","    min_ACER = 0.0\n","    min_APCER = 0.0\n","    min_BPCER = 0.0\n","    \n","    \n","    for d in data:\n","        threshold = d['map_score']\n","        \n","        type1 = len([s for s in data if s['map_score'] <= threshold and s['label'] == 1])\n","        type2 = len([s for s in data if s['map_score'] > threshold and s['label'] == 0])\n","        \n","        ACC = 1-(type1 + type2) / count\n","        APCER = type2 / num_fake\n","        BPCER = type1 / num_real\n","        ACER = (APCER + BPCER) / 2.0\n","        \n","        if ACER < min_error:\n","            min_error = ACER\n","            min_threshold = threshold\n","            min_ACC = ACC\n","            min_ACER = ACER\n","            min_APCER = APCER\n","            min_BPCER = min_BPCER\n","\n","    return min_threshold, min_ACC, min_APCER, min_BPCER, min_ACER\n","\n","\n","\n","def test_threshold_based(threshold, score_file):\n","    with open(score_file, 'r') as file:\n","        lines = file.readlines()\n","\n","    data = []\n","    count = 0.0\n","    num_real = 0.0\n","    num_fake = 0.0\n","    for line in lines:\n","        count += 1\n","        tokens = line.split()\n","        angle = float(tokens[0])\n","        type = int(tokens[1])\n","        data.append({'map_score': angle, 'label': type})\n","        if type==1:\n","            num_real += 1\n","        else:\n","            num_fake += 1\n","    \n"," \n","    type1 = len([s for s in data if s['map_score'] <= threshold and s['label'] == 1])\n","    type2 = len([s for s in data if s['map_score'] > threshold and s['label'] == 0])\n","    \n","    ACC = 1-(type1 + type2) / count\n","    APCER = type2 / num_fake\n","    BPCER = type1 / num_real\n","    ACER = (APCER + BPCER) / 2.0\n","    \n","    return ACC, APCER, BPCER, ACER\n","\n","\n","def get_err_threhold(fpr, tpr, threshold):\n","    RightIndex=(tpr+(1-fpr)-1); \n","    right_index = np.argmax(RightIndex)\n","    best_th = threshold[right_index]\n","    err = fpr[right_index]\n","\n","    differ_tpr_fpr_1=tpr+fpr-1.0\n","  \n","    right_index = np.argmin(np.abs(differ_tpr_fpr_1))\n","    best_th = threshold[right_index]\n","    err = fpr[right_index]    \n","\n","    return err, best_th\n","\n","def performances(map_score_val_filename, map_score_test_filename):\n","\n","    # val \n","    with open(map_score_val_filename, 'r') as file:\n","        lines = file.readlines()\n","    val_scores = []\n","    val_labels = []\n","    data = []\n","    count = 0.0\n","    num_real = 0.0\n","    num_fake = 0.0\n","    for line in lines:\n","        count += 1\n","        tokens = line.split()\n","        score = float(tokens[0])\n","        label = float(tokens[1])  #label = int(tokens[1])\n","        val_scores.append(score)\n","        val_labels.append(label)\n","        data.append({'map_score': score, 'label': label})\n","        if label==1:\n","            num_real += 1\n","        else:\n","            num_fake += 1\n","    \n","    fpr,tpr,threshold = roc_curve(val_labels, val_scores, pos_label=1)\n","    fnr = 1 - tpr\n","    val_err, val_threshold = get_err_threhold(fpr, tpr, threshold)\n","    \n","    type1 = len([s for s in data if s['map_score'] <= val_threshold and s['label'] == 1])\n","    type2 = len([s for s in data if s['map_score'] > val_threshold and s['label'] == 0])\n","    \n","    val_ACC = 1-(type1 + type2) / count\n","    val_APCER = type2 / num_fake\n","    val_BPCER = type1 / num_real\n","    val_ACER = (val_APCER + val_BPCER) / 2.0\n","    \n","    ## EER\n","    eer_threshold = threshold[np.nanargmin(np.absolute((fnr - fpr)))]\n","    val_EER = fpr[np.nanargmin(np.absolute((fnr - fpr)))]\n","    \n","    \n","    \n","    # test \n","    with open(map_score_test_filename, 'r') as file2:\n","        lines = file2.readlines()\n","    test_scores = []\n","    test_labels = []\n","    data = []\n","    count = 0.0\n","    num_real = 0.0\n","    num_fake = 0.0\n","    for line in lines:\n","        count += 1\n","        tokens = line.split()\n","        score = float(tokens[0])\n","        label = float(tokens[1])    #label = int(tokens[1])\n","        test_scores.append(score)\n","        test_labels.append(label)\n","        data.append({'map_score': score, 'label': label})\n","        if label==1:\n","            num_real += 1\n","        else:\n","            num_fake += 1\n","    \n","    # test based on val_threshold     \n","    type1 = len([s for s in data if s['map_score'] <= val_threshold and s['label'] == 1])\n","    type2 = len([s for s in data if s['map_score'] > val_threshold and s['label'] == 0])\n","    \n","    test_ACC = 1-(type1 + type2) / count\n","    test_APCER = type2 / num_fake\n","    test_BPCER = type1 / num_real\n","    test_ACER = (test_APCER + test_BPCER) / 2.0\n","    \n","    \n","    # test based on test_threshold     \n","    fpr_test,tpr_test,threshold_test = roc_curve(test_labels, test_scores, pos_label=1)\n","    fnr_test = 1 - tpr_test\n","    err_test, best_test_threshold = get_err_threhold(fpr_test, tpr_test, threshold_test)\n","    \n","    type1 = len([s for s in data if s['map_score'] <= best_test_threshold and s['label'] == 1])\n","    type2 = len([s for s in data if s['map_score'] > best_test_threshold and s['label'] == 0])\n","    \n","    test_threshold_ACC = 1-(type1 + type2) / count\n","    test_threshold_APCER = type2 / num_fake\n","    test_threshold_BPCER = type1 / num_real\n","    test_threshold_ACER = (test_threshold_APCER + test_threshold_BPCER) / 2.0\n","    \n","    ## EER\n","    eer_threshold = threshold_test[np.nanargmin(np.absolute((fnr_test - fpr_test)))]\n","    test_EER = fpr_test[np.nanargmin(np.absolute((fnr_test - fpr_test)))]\n","    \n","    return val_threshold, best_test_threshold, val_ACC, val_ACER, val_EER, test_ACC, test_APCER, test_BPCER, test_ACER, test_threshold_ACER, test_EER"]},{"cell_type":"code","execution_count":19,"id":"49c21d46","metadata":{"execution":{"iopub.execute_input":"2023-10-12T16:08:14.661666Z","iopub.status.busy":"2023-10-12T16:08:14.661439Z","iopub.status.idle":"2023-10-12T16:08:14.693043Z","shell.execute_reply":"2023-10-12T16:08:14.692177Z"},"papermill":{"duration":0.040881,"end_time":"2023-10-12T16:08:14.694733","exception":false,"start_time":"2023-10-12T16:08:14.653852","status":"completed"},"tags":[]},"outputs":[],"source":["def train_model():\n","    seed_everything(CFG['seed'])\n","    \n","    ## Log file\n","    log_file = open('_log.txt', 'w')\n","\n","    print(\"Liveness Detection Zalo AI 2022:\\n \")\n","\n","    log_file.write('Liveness Detection Zalo AI 2022:\\n ')\n","    log_file.flush()\n","\n","    print('train from scratch!\\n')\n","    log_file.write('train from scratch!\\n')\n","    log_file.flush()\n","    \n","    ## train val test split - use GroupShuffleSplit twice\n","    # Train + val_test first\n","    splitter = GroupShuffleSplit(test_size=.20, n_splits=2)\n","    split = splitter.split(df, groups=df['fname'])\n","    train_inds, test_inds = next(split)\n","    \n","    train = df.iloc[train_inds].reset_index(drop=True)\n","    test = df.iloc[test_inds].reset_index(drop=True)\n","    \n","    # And then Val + test\n","    splitter = GroupShuffleSplit(test_size=.50, n_splits=2)\n","    split = splitter.split(test, groups=test['fname'])\n","    val_inds, test_inds = next(split)\n","    \n","    val = df.iloc[val_inds].reset_index(drop=True)\n","    test = df.iloc[test_inds].reset_index(drop=True)\n","    \n","    \n","    ## Model\n","    model = CDCN(basic_conv=Conv2d_cd, theta=CFG['theta'])\n","    model = model.to(device)\n","    \n","    ## Optimizer\n","    lr = CFG['lr']\n","    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=CFG['weight_decay'])\n","    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=CFG['step_size'], gamma=CFG['gamma'])\n","    \n","    ## Loss function\n","    criterion_absolute_loss = nn.MSELoss().to(device)\n","    criterion_contrastive_loss = Contrast_depth_loss().to(device)\n","    \n","\n","\n","    ACER_save = 1.0\n","    \n","    for epoch in range(CFG['epochs']):  # loop over the dataset multiple times\n","        if (epoch + 1) % CFG['step_size'] == 0:\n","            lr *= CFG['gamma']\n","        \n","        loss_absolute = AvgrageMeter()\n","        loss_contra =  AvgrageMeter()\n","        \n","        ###########################################\n","        '''                train             '''\n","        ###########################################\n","        model.train()\n","        \n","        # train dataloader\n","        train_transform = transforms.Compose([RandomErasing(), RandomHorizontalFlip(),  ToTensor(), Cutout(), Normaliztion()])\n","        train_data = TrainDataset(train, img_dir, train_transform)\n","        dataloader_train = DataLoader(train_data, batch_size=CFG['train_bs'], shuffle=True, num_workers=CFG['num_workers'])\n","\n","        for i, sample_batched in enumerate(dataloader_train):\n","            # get the inputs\n","            inputs, binary_mask, spoof_label = sample_batched['image_x'].to(device), sample_batched['binary_mask'].to(device), sample_batched['spoofing_label'].to(device)\n","\n","            optimizer.zero_grad()\n","\n","            \n","            # forward + backward + optimize\n","            map_x, embedding, x_Block1, x_Block2, x_Block3, x_input =  model(inputs)\n","            \n","            absolute_loss = criterion_absolute_loss(map_x, binary_mask)\n","            contrastive_loss = criterion_contrastive_loss(map_x, binary_mask)\n","            \n","            loss =  absolute_loss + contrastive_loss\n","             \n","            loss.backward()\n","            \n","            optimizer.step()\n","            \n","            n = inputs.size(0)\n","            loss_absolute.update(absolute_loss.data, n)\n","            loss_contra.update(contrastive_loss.data, n)           \n","        \n","        scheduler.step()\n","        \n","        # whole epoch average\n","        print('epoch:%d, Train:  Absolute_Depth_loss= %.4f, Contrastive_Depth_loss= %.4f\\n' % (epoch + 1, loss_absolute.avg, loss_contra.avg))\n","        log_file.write('epoch:%d, Train: Absolute_Depth_loss= %.4f, Contrastive_Depth_loss= %.4f \\n' % (epoch + 1, loss_absolute.avg, loss_contra.avg))\n","        log_file.flush()\n","           \n","            \n","        epoch_test = 1\n","        if epoch >= 0 and epoch % epoch_test == epoch_test-1:    \n","            model.eval()\n","            \n","            with torch.no_grad():\n","                ###########################################\n","                '''                val             '''\n","                ###########################################\n","                # val for threshold\n","                val_transform = transforms.Compose([Normaliztion_valtest(), ToTensor_valtest()])\n","                val_data = ValDataset(val, img_dir, val_transform)\n","                dataloader_val = DataLoader(val_data, batch_size=CFG['valid_bs'], shuffle=False, num_workers=CFG['num_workers'])\n","                \n","                map_score_list = []\n","                \n","                for i, sample_batched in enumerate(dataloader_val):\n","                    # get the inputs\n","                    inputs, spoof_label = sample_batched['image_x'].to(device), sample_batched['spoofing_label'].to(device)\n","                    binary_mask = sample_batched['binary_mask'].to(device)\n","        \n","                    optimizer.zero_grad()\n","                    \n","                    \n","                    map_score = 0.0\n","                    for frame_t in range(inputs.shape[1]):\n","                        map_x, embedding, x_Block1, x_Block2, x_Block3, x_input =  model(inputs[:,frame_t,:,:,:])\n","                        score_norm = torch.sum(map_x)/torch.sum(binary_mask[:,frame_t,:,:])\n","                        map_score += score_norm\n","                    map_score = map_score/inputs.shape[1]\n","                    \n","                    if map_score>1:\n","                        map_score = 1.0\n","    \n","                    map_score_list.append('{} {}\\n'.format(map_score, spoof_label[0][0]))\n","                    \n","                map_score_val_filename = '_map_score_val_%d.txt'% (epoch + 1)\n","                with open(map_score_val_filename, 'w') as file:\n","                    file.writelines(map_score_list) \n","                    \n","                ###########################################\n","                '''                test             '''\n","                ###########################################\n","                # val for threshold\n","                test_transform = transforms.Compose([Normaliztion_valtest(), ToTensor_valtest()])\n","                test_data = ValDataset(test, img_dir, val_transform)\n","                dataloader_test = DataLoader(test_data, batch_size=CFG['valid_bs'], shuffle=False, num_workers=CFG['num_workers'])\n","                \n","                map_score_list = []\n","                \n","                for i, sample_batched in enumerate(dataloader_test):\n","                    # get the inputs\n","                    inputs, spoof_label = sample_batched['image_x'].to(device), sample_batched['spoofing_label'].to(device)\n","                    binary_mask = sample_batched['binary_mask'].to(device)\n","        \n","                    optimizer.zero_grad()\n","                    \n","                    \n","                    map_score = 0.0\n","                    for frame_t in range(inputs.shape[1]):\n","                        map_x, embedding, x_Block1, x_Block2, x_Block3, x_input =  model(inputs[:,frame_t,:,:,:])\n","                        score_norm = torch.sum(map_x)/torch.sum(binary_mask[:,frame_t,:,:])\n","                        map_score += score_norm\n","                    map_score = map_score/inputs.shape[1]\n","                    \n","                    if map_score>1:\n","                        map_score = 1.0\n","    \n","                    map_score_list.append('{} {}\\n'.format(map_score, spoof_label[0][0]))\n","                    \n","                map_score_test_filename = '_map_score_test_%d.txt'% (epoch + 1)\n","                with open(map_score_test_filename, 'w') as file:\n","                    file.writelines(map_score_list)\n","                \n","                #############################################################     \n","                #       performance measurement\n","                #############################################################     \n","                val_threshold, best_test_threshold, val_ACC, val_ACER, val_EER, test_ACC, test_APCER, test_BPCER, test_ACER, test_threshold_ACER, test_EER = performances(map_score_val_filename, map_score_test_filename)\n","                \n","                print('epoch:%d, Val:  val_threshold= %.4f, val_ACC= %.4f, val_ACER= %.4f, val_EER= %.4f' % (epoch + 1, val_threshold, val_ACC, val_ACER, val_EER))\n","                log_file.write('\\n epoch:%d, Val:  val_threshold= %.4f, val_ACC= %.4f, val_ACER= %.4f, val_EER= %.4f \\n' % (epoch + 1, val_threshold, val_ACC, val_ACER, val_EER))\n","              \n","                print('epoch:%d, Test:  ACC= %.4f, APCER= %.4f, BPCER= %.4f, ACER= %.4f, EER= %.4f' % (epoch + 1, test_ACC, test_APCER, test_BPCER, test_ACER, test_EER))\n","                log_file.write('epoch:%d, Test:  ACC= %.4f, APCER= %.4f, BPCER= %.4f, ACER= %.4f, EER= %.4f \\n' % (epoch + 1, test_ACC, test_APCER, test_BPCER, test_ACER, test_EER))\n","                \n","                log_file.flush()\n","\n","            \n","            # save the model until the next improvement     \n","            torch.save(model.state_dict(), '_%d.pkl' % (epoch + 1))\n","\n","\n","    print('Finished Training')\n","    log_file.close()"]},{"cell_type":"markdown","id":"377ee4be","metadata":{"papermill":{"duration":0.004597,"end_time":"2023-10-12T16:08:14.704047","exception":false,"start_time":"2023-10-12T16:08:14.69945","status":"completed"},"tags":[]},"source":["## Main"]},{"cell_type":"code","execution_count":20,"id":"39924708","metadata":{"execution":{"iopub.execute_input":"2023-10-12T16:08:14.715173Z","iopub.status.busy":"2023-10-12T16:08:14.714384Z","iopub.status.idle":"2023-10-13T01:29:25.17769Z","shell.execute_reply":"2023-10-13T01:29:25.176382Z"},"papermill":{"duration":33670.470966,"end_time":"2023-10-13T01:29:25.179784","exception":false,"start_time":"2023-10-12T16:08:14.708818","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Liveness Detection Zalo AI 2022:\n"," \n","train from scratch!\n","\n","epoch:1, Train:  Absolute_Depth_loss= 0.3560, Contrastive_Depth_loss= 0.0341\n","\n","epoch:1, Val:  val_threshold= 0.4101, val_ACC= 0.4530, val_ACER= 0.5469, val_EER= 0.5636\n","epoch:1, Test:  ACC= 0.4701, APCER= 0.5254, BPCER= 0.5345, ACER= 0.5300, EER= 0.5254\n","epoch:2, Train:  Absolute_Depth_loss= 0.2593, Contrastive_Depth_loss= 0.0173\n","\n","epoch:2, Val:  val_threshold= 0.5011, val_ACC= 0.7521, val_ACER= 0.2472, val_EER= 0.2545\n","epoch:2, Test:  ACC= 0.6752, APCER= 0.3220, BPCER= 0.3276, ACER= 0.3248, EER= 0.3051\n","epoch:3, Train:  Absolute_Depth_loss= 0.2397, Contrastive_Depth_loss= 0.0093\n","\n","epoch:3, Val:  val_threshold= 0.5059, val_ACC= 0.7778, val_ACER= 0.2220, val_EER= 0.2182\n","epoch:3, Test:  ACC= 0.7350, APCER= 0.2712, BPCER= 0.2586, ACER= 0.2649, EER= 0.2712\n","epoch:4, Train:  Absolute_Depth_loss= 0.2297, Contrastive_Depth_loss= 0.0081\n","\n","epoch:4, Val:  val_threshold= 0.4933, val_ACC= 0.7949, val_ACER= 0.2048, val_EER= 0.2182\n","epoch:4, Test:  ACC= 0.7521, APCER= 0.2881, BPCER= 0.2069, ACER= 0.2475, EER= 0.2373\n","epoch:5, Train:  Absolute_Depth_loss= 0.2196, Contrastive_Depth_loss= 0.0077\n","\n","epoch:5, Val:  val_threshold= 0.4827, val_ACC= 0.8291, val_ACER= 0.1705, val_EER= 0.1818\n","epoch:5, Test:  ACC= 0.7607, APCER= 0.2373, BPCER= 0.2414, ACER= 0.2393, EER= 0.2373\n","epoch:6, Train:  Absolute_Depth_loss= 0.2107, Contrastive_Depth_loss= 0.0074\n","\n","epoch:6, Val:  val_threshold= 0.4899, val_ACC= 0.8376, val_ACER= 0.1614, val_EER= 0.1818\n","epoch:6, Test:  ACC= 0.7607, APCER= 0.2203, BPCER= 0.2586, ACER= 0.2395, EER= 0.2373\n","epoch:7, Train:  Absolute_Depth_loss= 0.2053, Contrastive_Depth_loss= 0.0077\n","\n","epoch:7, Val:  val_threshold= 0.4761, val_ACC= 0.8462, val_ACER= 0.1534, val_EER= 0.1455\n","epoch:7, Test:  ACC= 0.7863, APCER= 0.2203, BPCER= 0.2069, ACER= 0.2136, EER= 0.2203\n","epoch:8, Train:  Absolute_Depth_loss= 0.1942, Contrastive_Depth_loss= 0.0077\n","\n","epoch:8, Val:  val_threshold= 0.4694, val_ACC= 0.8803, val_ACER= 0.1180, val_EER= 0.0909\n","epoch:8, Test:  ACC= 0.8034, APCER= 0.1864, BPCER= 0.2069, ACER= 0.1967, EER= 0.1864\n","epoch:9, Train:  Absolute_Depth_loss= 0.1871, Contrastive_Depth_loss= 0.0079\n","\n","epoch:9, Val:  val_threshold= 0.5716, val_ACC= 0.8974, val_ACER= 0.1019, val_EER= 0.1091\n","epoch:9, Test:  ACC= 0.8291, APCER= 0.1864, BPCER= 0.1552, ACER= 0.1708, EER= 0.1864\n","epoch:10, Train:  Absolute_Depth_loss= 0.1774, Contrastive_Depth_loss= 0.0077\n","\n","epoch:10, Val:  val_threshold= 0.5022, val_ACC= 0.8889, val_ACER= 0.1110, val_EER= 0.1273\n","epoch:10, Test:  ACC= 0.8547, APCER= 0.1525, BPCER= 0.1379, ACER= 0.1452, EER= 0.1356\n","epoch:11, Train:  Absolute_Depth_loss= 0.1662, Contrastive_Depth_loss= 0.0078\n","\n","epoch:11, Val:  val_threshold= 0.4992, val_ACC= 0.8889, val_ACER= 0.1110, val_EER= 0.1273\n","epoch:11, Test:  ACC= 0.8632, APCER= 0.1864, BPCER= 0.0862, ACER= 0.1363, EER= 0.1017\n","epoch:12, Train:  Absolute_Depth_loss= 0.1630, Contrastive_Depth_loss= 0.0076\n","\n","epoch:12, Val:  val_threshold= 0.5589, val_ACC= 0.8974, val_ACER= 0.1019, val_EER= 0.1091\n","epoch:12, Test:  ACC= 0.8547, APCER= 0.1186, BPCER= 0.1724, ACER= 0.1455, EER= 0.1356\n","epoch:13, Train:  Absolute_Depth_loss= 0.1586, Contrastive_Depth_loss= 0.0078\n","\n","epoch:13, Val:  val_threshold= 0.4398, val_ACC= 0.9145, val_ACER= 0.0848, val_EER= 0.0727\n","epoch:13, Test:  ACC= 0.8974, APCER= 0.1356, BPCER= 0.0690, ACER= 0.1023, EER= 0.1017\n","epoch:14, Train:  Absolute_Depth_loss= 0.1482, Contrastive_Depth_loss= 0.0078\n","\n","epoch:14, Val:  val_threshold= 0.5039, val_ACC= 0.9060, val_ACER= 0.0938, val_EER= 0.1091\n","epoch:14, Test:  ACC= 0.8974, APCER= 0.1356, BPCER= 0.0690, ACER= 0.1023, EER= 0.1017\n","epoch:15, Train:  Absolute_Depth_loss= 0.1387, Contrastive_Depth_loss= 0.0079\n","\n","epoch:15, Val:  val_threshold= 0.5029, val_ACC= 0.9231, val_ACER= 0.0767, val_EER= 0.0909\n","epoch:15, Test:  ACC= 0.9487, APCER= 0.0847, BPCER= 0.0172, ACER= 0.0510, EER= 0.0508\n","Finished Training\n"]}],"source":["if __name__ == \"__main__\":\n","    train_model()"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"papermill":{"default_parameters":{},"duration":33681.305354,"end_time":"2023-10-13T01:29:27.244019","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2023-10-12T16:08:05.938665","version":"2.4.0"}},"nbformat":4,"nbformat_minor":5}